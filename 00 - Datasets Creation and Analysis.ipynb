{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install fasttext\n",
    "#!wget -O ./data/lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "import fasttext\n",
    "import re\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [WH+TW] Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "def get_whisper(filename):\n",
    "    serial = []\n",
    "    text = []\n",
    "    for line in open(filename, 'r'):\n",
    "        new_data = json.loads(line)\n",
    "        serial.append(new_data[\"serial\"])\n",
    "        text.append(new_data[\"text\"])\n",
    "    return pd.DataFrame({\"serial\": serial, \"text\": text})\n",
    "\n",
    "def get_twitter(filename):\n",
    "    text = []\n",
    "    for line in open(filename, 'r'):\n",
    "        splitted = line.split('\\t')\n",
    "        if len(splitted) == 4:\n",
    "            text.append(splitted[2])\n",
    "    return pd.DataFrame({\"text\": text})\n",
    "\n",
    "def is_english(text):\n",
    "    dl = lang_model.predict(text)[0][0][-2:]\n",
    "    return dl == 'en'\n",
    "\n",
    "def replace_if_lang(df, text):\n",
    "    def func(row):\n",
    "        if is_english(row[\"text\"]):\n",
    "            return row[\"text\"]\n",
    "        return \"\"\n",
    "    df['text'] = df.apply(func, axis=1).tolist()\n",
    "    return df\n",
    "\n",
    "def replace_in_df(df, regex, text):\n",
    "    def func(row):\n",
    "        return re.sub(regex, text, row[\"text\"])\n",
    "    df['text'] = df.apply(func, axis=1).tolist()\n",
    "    return df\n",
    "\n",
    "def clean_tweets(df, min_length=8):\n",
    "    # delete the entire tweet (1/2)\n",
    "    df = replace_in_df(df, r\".*RT @.*\", \"\") # retweets\n",
    "    df = replace_in_df(df, r\".*\\[\\[.*\", \"\") # placeholders\n",
    "    # delete only a part of the tweet\n",
    "    df = replace_in_df(df, r\"http(s?)://\\S+\", \"\") # links\n",
    "    df = replace_in_df(df, r\"@\\S+\", \"\") # mentions\n",
    "    df = replace_in_df(df, r\"#\\S+\", \"\") # hashtags\n",
    "    df = replace_in_df(df, r\"\\n\", \" \") # newline\n",
    "    df = replace_in_df(df, r\"  +\", \" \") # multiple spaces\n",
    "    df = replace_in_df(df, r\"^ \", \"\") # space at the begin of line\n",
    "    df = replace_in_df(df, r\" $\", \"\") # space at the end of line\n",
    "    # delete the entire tweet (2/2)\n",
    "    df = replace_if_lang(df, \"\") # texts not in English\n",
    "    df = df[df['text'].map(len) >= min_length] # short text\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def get_datasets(whisper_filename, twitter_filename):\n",
    "    def get_dataset(filename, func, cls):\n",
    "        data = func(filename)\n",
    "        data[\"class\"] = cls\n",
    "        data = data[['text', 'class']]\n",
    "        return clean_tweets(data)\n",
    "    data_w = get_dataset(whisper_filename, get_whisper, \"sens\")\n",
    "    data_t = get_dataset(twitter_filename, get_twitter, \"ns\")\n",
    "    return data_w, data_t\n",
    "\n",
    "def save_samples_data(num_samples, df_sens, df_ns, num_sens, num_ns, filename_prefix):\n",
    "    for count in range(num_samples):\n",
    "        df_sens[(num_sens*count):(num_sens*(count+1))].append(df_ns[(num_ns*count):(num_ns*(count+1))]).sample(frac=1, random_state=2**count).reset_index(drop=True).to_csv(filename_prefix+(\"0\"+str(count+1))[-2:]+\".csv\", index=False)\n",
    "    return\n",
    "\n",
    "lang_model = fasttext.load_model('./data/lid.176.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sens, df_ns = get_datasets('/data/whisper/final-anonymized.jun14.jun16.whisper-part-000.json', \n",
    "                '/data/twitter-cikm-2010/test_set_tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My mom just sold my fucking bed without tellin...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me and this guy had a full heart to heart conv...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NJ women message me</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who am I even anymore?</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm a post man. N would love a lonely houswife...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595080</th>\n",
       "      <td>because of tumblr girls I wish I wasn't my race</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595081</th>\n",
       "      <td>Still looking to find that first attractive gi...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595082</th>\n",
       "      <td>Just dropped my phone on my face three times w...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595083</th>\n",
       "      <td>I never really appreciated my mom until, as a ...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595084</th>\n",
       "      <td>Just found out one of my close friends liked a...</td>\n",
       "      <td>sens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3228927 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text class\n",
       "0        My mom just sold my fucking bed without tellin...  sens\n",
       "1        Me and this guy had a full heart to heart conv...  sens\n",
       "3                                      NJ women message me  sens\n",
       "4                                   Who am I even anymore?  sens\n",
       "5        I'm a post man. N would love a lonely houswife...  sens\n",
       "...                                                    ...   ...\n",
       "3595080    because of tumblr girls I wish I wasn't my race  sens\n",
       "3595081  Still looking to find that first attractive gi...  sens\n",
       "3595082  Just dropped my phone on my face three times w...  sens\n",
       "3595083  I never really appreciated my mom until, as a ...  sens\n",
       "3595084  Just found out one of my close friends liked a...  sens\n",
       "\n",
       "[3228927 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ok today I have to find something to wear for ...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am glad I'm having this show but I can't wai...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honestly I don't even know what's going on any...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hey sorry I'm sitting infront of this sewing m...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sitting infront of this sewing machine ... I d...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108193</th>\n",
       "      <td>Where's the spot</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108197</th>\n",
       "      <td>I need 2 labtops. 1 for playin music n 1 4 the...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108198</th>\n",
       "      <td>whip like a slave is that song u was askin bou...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108201</th>\n",
       "      <td>das rite b4 eastternparkway rite (comin from m...</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108207</th>\n",
       "      <td>lmaooo yea dat shit is crazy</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3805606 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text class\n",
       "0        Ok today I have to find something to wear for ...    ns\n",
       "1        I am glad I'm having this show but I can't wai...    ns\n",
       "2        Honestly I don't even know what's going on any...    ns\n",
       "3        hey sorry I'm sitting infront of this sewing m...    ns\n",
       "4        Sitting infront of this sewing machine ... I d...    ns\n",
       "...                                                    ...   ...\n",
       "5108193                                   Where's the spot    ns\n",
       "5108197  I need 2 labtops. 1 for playin music n 1 4 the...    ns\n",
       "5108198  whip like a slave is that song u was askin bou...    ns\n",
       "5108201  das rite b4 eastternparkway rite (comin from m...    ns\n",
       "5108207                       lmaooo yea dat shit is crazy    ns\n",
       "\n",
       "[3805606 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_samples_data(10, df_sens.sample(frac=1, replace=False, random_state=512), df_ns.sample(frac=1, replace=False, random_state=256), 3336, 5429, \"./data/sample_ann2_\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "\n",
    "def get_data(path, lim=2):\n",
    "    data = pd.read_csv(path, sep=',', error_bad_lines=False)\n",
    "    res = {\"uri\":[], \"text\":[], \"class\":[]}\n",
    "    for tag in data.columns[2:]:\n",
    "        tmp = data[data[tag] >= lim]\n",
    "        res[\"uri\"] += tmp[\"uri\"].tolist()\n",
    "        res[\"text\"] += tmp[\"text\"].tolist()\n",
    "        res[\"class\"] += [tag]*len(tmp)\n",
    "    res = pd.DataFrame(res).sort_values(by=\"uri\")\n",
    "    return res\n",
    "\n",
    "# keep only \"Sensibile\" and \"Non sensibile\" as \"sens\" and \"ns\"\n",
    "def get_two_classes(df):\n",
    "    res = df[df[\"class\"].isin([\"Sensibile\", \"Non sensibile\"])].copy()\n",
    "    res.loc[res[\"class\"] == \"Sensibile\", \"class\"] = \"sens\"\n",
    "    res.loc[res[\"class\"] == \"Non sensibile\", \"class\"] = \"ns\"\n",
    "    return res\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = SnowballStemmer('english')\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    text = word_tokenize(text)  # tokenize text\n",
    "    ## Remove english stop words\n",
    "    #stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    text = [w for w in text if w not in STOPWORDS and w.lower() not in STOPWORDS]  # remove stop words\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(\"[^a-zA-z0-9\\s]\", \" \", text)\n",
    "    # text = re.sub(\"[^a-zA-Z#]\", \" \", text)\n",
    "    text = word_tokenize(text)  # tokenize text\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "#stem_or_lemma: 1 for stemming, 2 for lemmatization, other for none\n",
    "def clean_text(text, remove_sw=False, stem_or_lemma=0):\n",
    "    def remove_url(txt):\n",
    "        return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "    \n",
    "    res = remove_url(text).lower().split()\n",
    "    # Remove english stop words\n",
    "    if remove_sw:\n",
    "        #stop_words = set(stopwords.words(\"english\"))\n",
    "        res = [word for word in res if not word in STOPWORDS]\n",
    "    if stem_or_lemma == 1: # stemming\n",
    "        #stemmer = SnowballStemmer('english')\n",
    "        res = [STEMMER.stem(word) for word in res]\n",
    "        #res = \" \".join([stemmer.stem(word) for word in res])\n",
    "    elif stem_or_lemma == 2: # lemmatization\n",
    "        #lemmatizer = WordNetLemmatizer()\n",
    "        res = [LEMMATIZER.lemmatize(word, pos='v') for word in res]\n",
    "\n",
    "    return res#.split() #remove_url(text).lower().split()\n",
    "\n",
    "def get_df_stats(df):\n",
    "    df_mean = df.groupby(\"word\").mean().reset_index().sort_values(by=\"all\", ascending=False).reset_index(drop=True)\n",
    "    df_mean.columns = [\"word\"]+[x+\"_mean\" for x in df_mean.columns[1:]]\n",
    "    df_std = df.groupby(\"word\").std().fillna(0).reset_index()\n",
    "    df_std.columns = [\"word\"]+[x+\"_std\" for x in df_std.columns[1:]]\n",
    "    return df_mean.merge(df_std)\n",
    "\n",
    "def get_df_stats_2(data, main_col):\n",
    "    words_sens = list(itertools.chain(*data[data['class'] == 'sens'][main_col].values.tolist()))\n",
    "    #words_ns = list(itertools.chain(*data[data['class'] == 'ns'][main_col].values.tolist()))\n",
    "    words_all = list(itertools.chain(*data[main_col].values.tolist()))\n",
    "    wordcount_sens = collections.Counter(words_sens)\n",
    "    #wordcount_ns = collections.Counter(words_ns)\n",
    "    wordcount_all = collections.Counter(words_all)\n",
    "    print(\"Top words\", wordcount_all.most_common(20),\"\\n\")\n",
    "    top_wc_dict = {\"word\": [], \"all\": [], \"sens\": [], \"ns\": [], \"perc_sens\": [], \"perc_ns\": []}\n",
    "    for word in wordcount_all:\n",
    "        num_all = wordcount_all[word]\n",
    "        num_sens = wordcount_sens[word]\n",
    "        perc_sens = round((num_sens*100)/num_all,2)\n",
    "        #print(item[0], item[1], perc_sens, round((100.0-perc_sens), 2))\n",
    "        top_wc_dict[\"word\"].append(word)\n",
    "        top_wc_dict[\"all\"].append(num_all)\n",
    "        top_wc_dict[\"sens\"].append(num_sens)\n",
    "        top_wc_dict[\"ns\"].append(num_all-num_sens)\n",
    "        top_wc_dict[\"perc_sens\"].append(perc_sens)\n",
    "        top_wc_dict[\"perc_ns\"].append(round((100.0-perc_sens), 2))\n",
    "    return pd.DataFrame(top_wc_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sens] Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words [('propnam', 1062), ('go', 775), ('get', 651), ('day', 610), ('im', 493), ('like', 490), ('work', 458), ('time', 447), ('one', 433), ('love', 406), ('back', 364), ('want', 361), ('new', 358), ('today', 346), ('good', 343), ('think', 327), ('make', 306), ('know', 301), ('need', 295), ('got', 290)] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>all</th>\n",
       "      <th>sens</th>\n",
       "      <th>ns</th>\n",
       "      <th>perc_sens</th>\n",
       "      <th>perc_ns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>propnam</td>\n",
       "      <td>1062</td>\n",
       "      <td>580</td>\n",
       "      <td>482</td>\n",
       "      <td>54.61</td>\n",
       "      <td>45.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>775</td>\n",
       "      <td>426</td>\n",
       "      <td>349</td>\n",
       "      <td>54.97</td>\n",
       "      <td>45.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get</td>\n",
       "      <td>651</td>\n",
       "      <td>316</td>\n",
       "      <td>335</td>\n",
       "      <td>48.54</td>\n",
       "      <td>51.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>610</td>\n",
       "      <td>316</td>\n",
       "      <td>294</td>\n",
       "      <td>51.80</td>\n",
       "      <td>48.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im</td>\n",
       "      <td>493</td>\n",
       "      <td>253</td>\n",
       "      <td>240</td>\n",
       "      <td>51.32</td>\n",
       "      <td>48.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>converg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>augustand</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>uchicon</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>uchicago</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>sniper</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11829 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word   all  sens   ns  perc_sens  perc_ns\n",
       "0        propnam  1062   580  482      54.61    45.39\n",
       "1             go   775   426  349      54.97    45.03\n",
       "2            get   651   316  335      48.54    51.46\n",
       "3            day   610   316  294      51.80    48.20\n",
       "4             im   493   253  240      51.32    48.68\n",
       "...          ...   ...   ...  ...        ...      ...\n",
       "11824    converg     1     1    0     100.00     0.00\n",
       "11825  augustand     1     1    0     100.00     0.00\n",
       "11826    uchicon     1     1    0     100.00     0.00\n",
       "11827   uchicago     1     1    0     100.00     0.00\n",
       "11828     sniper     1     0    1       0.00   100.00\n",
       "\n",
       "[11829 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(\"./data/annotation_results.csv\", lim=2)\n",
    "data = get_two_classes(data)\n",
    "data['text_cleaned_stem'] = data['text'].map(lambda x: clean_text(x, remove_sw=True, stem_or_lemma=1))\n",
    "data = get_df_stats_2(data, 'text_cleaned_stem').sort_values(by=\"all\", ascending=False).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  TOP 20 all words\n",
      "       word   all  perc_sens  perc_ns\n",
      "0   propnam  1062      54.61    45.39\n",
      "1        go   775      54.97    45.03\n",
      "2       get   651      48.54    51.46\n",
      "3       day   610      51.80    48.20\n",
      "4        im   493      51.32    48.68\n",
      "5      like   490      32.24    67.76\n",
      "6      work   458      64.19    35.81\n",
      "7      time   447      41.61    58.39\n",
      "8       one   433      30.95    69.05\n",
      "9      love   406      32.27    67.73\n",
      "10     back   364      64.01    35.99\n",
      "11     want   361      41.83    58.17\n",
      "12      new   358      36.03    63.97\n",
      "13    today   346      54.62    45.38\n",
      "14     good   343      40.82    59.18\n",
      "15    think   327      31.80    68.20\n",
      "16     make   306      35.29    64.71\n",
      "17     know   301      38.87    61.13\n",
      "18     need   295      40.00    60.00\n",
      "19      got   290      51.03    48.97\n",
      "\n",
      "##  TOP 20 sensible words\n",
      "        word  sens  %_sens   %_ns\n",
      "0    propnam   580   54.61  45.39\n",
      "1         go   426   54.97  45.03\n",
      "2        get   316   48.54  51.46\n",
      "3        day   316   51.80  48.20\n",
      "6       work   294   64.19  35.81\n",
      "4         im   253   51.32  48.68\n",
      "10      back   233   64.01  35.99\n",
      "29      home   192   78.05  21.95\n",
      "13     today   189   54.62  45.38\n",
      "7       time   186   41.61  58.39\n",
      "28  tomorrow   181   73.28  26.72\n",
      "20     night   168   58.33  41.67\n",
      "5       like   158   32.24  67.76\n",
      "11      want   151   41.83  58.17\n",
      "19       got   148   51.03  48.97\n",
      "31   tonight   147   63.91  36.09\n",
      "14      good   140   40.82  59.18\n",
      "8        one   134   30.95  69.05\n",
      "9       love   131   32.27  67.73\n",
      "38      week   130   64.04  35.96\n",
      "\n",
      "##  TOP 20 not sensible words\n",
      "       word   ns  %_sens   %_ns\n",
      "0   propnam  482   54.61  45.39\n",
      "1        go  349   54.97  45.03\n",
      "2       get  335   48.54  51.46\n",
      "5      like  332   32.24  67.76\n",
      "8       one  299   30.95  69.05\n",
      "3       day  294   51.80  48.20\n",
      "9      love  275   32.27  67.73\n",
      "7      time  261   41.61  58.39\n",
      "4        im  240   51.32  48.68\n",
      "12      new  229   36.03  63.97\n",
      "15    think  223   31.80  68.20\n",
      "11     want  210   41.83  58.17\n",
      "14     good  203   40.82  59.18\n",
      "16     make  198   35.29  64.71\n",
      "22     dont  187   32.73  67.27\n",
      "27    peopl  186   25.90  74.10\n",
      "17     know  184   38.87  61.13\n",
      "18     need  177   40.00  60.00\n",
      "30     life  166   30.25  69.75\n",
      "6      work  164   64.19  35.81\n",
      "\n",
      "##  TOP 20 percentage sensible words\n",
      "         word  all  %_sens   %_ns\n",
      "29       home  246   78.05  21.95\n",
      "97       sick  110   77.27  22.73\n",
      "28   tomorrow  247   73.28  26.72\n",
      "96   birthday  110   65.45  34.55\n",
      "71      class  136   65.44  34.56\n",
      "6        work  458   64.19  35.81\n",
      "38       week  203   64.04  35.96\n",
      "10       back  364   64.01  35.99\n",
      "31    tonight  230   63.91  36.09\n",
      "53    weekend  166   63.86  36.14\n",
      "86       miss  120   63.33  36.67\n",
      "58     school  160   61.25  38.75\n",
      "48      sleep  175   60.57  39.43\n",
      "84       hous  121   60.33  39.67\n",
      "63       wait  151   60.26  39.74\n",
      "20      night  288   58.33  41.67\n",
      "55       hour  163   58.28  41.72\n",
      "67      excit  139   58.27  41.73\n",
      "100      head  105   57.14  42.86\n",
      "104       yay  102   56.86  43.14\n",
      "\n",
      "##  TOP 20 percentage not sensible words\n",
      "         word  all  %_sens   %_ns\n",
      "83   facebook  122   13.93  86.07\n",
      "78      world  127   18.90  81.10\n",
      "102     alway  102   23.53  76.47\n",
      "81      never  123   24.39  75.61\n",
      "27      peopl  251   25.90  74.10\n",
      "108     write  100   26.00  74.00\n",
      "70   christma  136   27.21  72.79\n",
      "69        say  137   29.20  70.80\n",
      "74       ever  133   29.32  70.68\n",
      "30       life  238   30.25  69.75\n",
      "82       read  122   30.33  69.67\n",
      "45      would  188   30.85  69.15\n",
      "8         one  433   30.95  69.05\n",
      "88       done  119   31.09  68.91\n",
      "80     someth  125   31.20  68.80\n",
      "15      think  327   31.80  68.20\n",
      "5        like  490   32.24  67.76\n",
      "9        love  406   32.27  67.73\n",
      "66        let  141   32.62  67.38\n",
      "22       dont  278   32.73  67.27\n"
     ]
    }
   ],
   "source": [
    "main_cols = [\"word\", \"all\", \"perc_sens\", \"perc_ns\"]\n",
    "rename_cols = {\"perc_sens\":\"%_sens\", \"perc_ns\":\"%_ns\"}\n",
    "top_values = 20\n",
    "print(\"##  TOP\", top_values, \"all words\")\n",
    "print(data[main_cols][:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"sensible words\")\n",
    "main_cols = [\"word\", \"sens\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data.sort_values(by=\"sens\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"not sensible words\")\n",
    "main_cols = [\"word\", \"ns\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data.sort_values(by=\"ns\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"percentage sensible words\")\n",
    "main_cols = [\"word\", \"all\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data[data[\"all\"]>=100].sort_values(by=\"perc_sens\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"percentage not sensible words\")\n",
    "main_cols = [\"word\", \"all\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data[data[\"all\"]>=100].sort_values(by=\"perc_ns\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words [('propnam', 518), ('go', 382), ('get', 308), ('day', 267), ('like', 223), ('im', 206), ('one', 203), ('love', 200), ('time', 199), ('work', 187), ('back', 187), ('new', 168), ('see', 165), ('today', 160), ('want', 155), ('peopl', 155), ('good', 152), ('think', 152), ('dont', 151), ('make', 150)] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>all</th>\n",
       "      <th>sens</th>\n",
       "      <th>ns</th>\n",
       "      <th>perc_sens</th>\n",
       "      <th>perc_ns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>propnam</td>\n",
       "      <td>518</td>\n",
       "      <td>323</td>\n",
       "      <td>195</td>\n",
       "      <td>62.36</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go</td>\n",
       "      <td>382</td>\n",
       "      <td>234</td>\n",
       "      <td>148</td>\n",
       "      <td>61.26</td>\n",
       "      <td>38.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get</td>\n",
       "      <td>308</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>267</td>\n",
       "      <td>138</td>\n",
       "      <td>129</td>\n",
       "      <td>51.69</td>\n",
       "      <td>48.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>223</td>\n",
       "      <td>64</td>\n",
       "      <td>159</td>\n",
       "      <td>28.70</td>\n",
       "      <td>71.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7303</th>\n",
       "      <td>disciplin</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7304</th>\n",
       "      <td>ember</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7305</th>\n",
       "      <td>alight</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7306</th>\n",
       "      <td>suprem</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7307</th>\n",
       "      <td>sniper</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7308 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  all  sens   ns  perc_sens  perc_ns\n",
       "0       propnam  518   323  195      62.36    37.64\n",
       "1            go  382   234  148      61.26    38.74\n",
       "2           get  308   154  154      50.00    50.00\n",
       "3           day  267   138  129      51.69    48.31\n",
       "4          like  223    64  159      28.70    71.30\n",
       "...         ...  ...   ...  ...        ...      ...\n",
       "7303  disciplin    1     0    1       0.00   100.00\n",
       "7304      ember    1     0    1       0.00   100.00\n",
       "7305     alight    1     0    1       0.00   100.00\n",
       "7306     suprem    1     1    0     100.00     0.00\n",
       "7307     sniper    1     0    1       0.00   100.00\n",
       "\n",
       "[7308 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(\"./data/annotation_results.csv\", lim=3)\n",
    "data = get_two_classes(data)\n",
    "data['text_cleaned_stem'] = data['text'].map(lambda x: clean_text(x, remove_sw=True, stem_or_lemma=1))\n",
    "data = get_df_stats_2(data, 'text_cleaned_stem').sort_values(by=\"all\", ascending=False).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  TOP 20 all words\n",
      "       word  all  perc_sens  perc_ns\n",
      "0   propnam  518      62.36    37.64\n",
      "1        go  382      61.26    38.74\n",
      "2       get  308      50.00    50.00\n",
      "3       day  267      51.69    48.31\n",
      "4      like  223      28.70    71.30\n",
      "5        im  206      52.91    47.09\n",
      "6       one  203      23.65    76.35\n",
      "7      love  200      26.00    74.00\n",
      "8      time  199      44.22    55.78\n",
      "9      work  187      72.73    27.27\n",
      "10     back  187      70.59    29.41\n",
      "11      new  168      32.14    67.86\n",
      "12      see  165      48.48    51.52\n",
      "13    today  160      57.50    42.50\n",
      "14    peopl  155      17.42    82.58\n",
      "15     want  155      40.00    60.00\n",
      "16    think  152      24.34    75.66\n",
      "17     good  152      36.84    63.16\n",
      "18     dont  151      25.17    74.83\n",
      "19     make  150      28.67    71.33\n",
      "\n",
      "##  TOP 20 sensible words\n",
      "        word  sens  %_sens   %_ns\n",
      "0    propnam   323   62.36  37.64\n",
      "1         go   234   61.26  38.74\n",
      "2        get   154   50.00  50.00\n",
      "3        day   138   51.69  48.31\n",
      "9       work   136   72.73  27.27\n",
      "10      back   132   70.59  29.41\n",
      "20      home   123   86.62  13.38\n",
      "5         im   109   52.91  47.09\n",
      "26  tomorrow    99   81.82  18.18\n",
      "13     today    92   57.50  42.50\n",
      "8       time    88   44.22  55.78\n",
      "12       see    80   48.48  51.52\n",
      "33   tonight    79   78.22  21.78\n",
      "36   weekend    71   73.96  26.04\n",
      "29     night    68   59.13  40.87\n",
      "22      come    67   48.91  51.09\n",
      "52      sick    66   82.50  17.50\n",
      "4       like    64   28.70  71.30\n",
      "27       got    62   52.99  47.01\n",
      "15      want    62   40.00  60.00\n",
      "\n",
      "##  TOP 20 not sensible words\n",
      "       word   ns  %_sens   %_ns\n",
      "0   propnam  195   62.36  37.64\n",
      "4      like  159   28.70  71.30\n",
      "6       one  155   23.65  76.35\n",
      "2       get  154   50.00  50.00\n",
      "7      love  148   26.00  74.00\n",
      "1        go  148   61.26  38.74\n",
      "3       day  129   51.69  48.31\n",
      "14    peopl  128   17.42  82.58\n",
      "16    think  115   24.34  75.66\n",
      "11      new  114   32.14  67.86\n",
      "18     dont  113   25.17  74.83\n",
      "8      time  111   44.22  55.78\n",
      "19     make  107   28.67  71.33\n",
      "5        im   97   52.91  47.09\n",
      "17     good   96   36.84  63.16\n",
      "21     know   95   32.14  67.86\n",
      "15     want   93   40.00  60.00\n",
      "23    happi   91   33.09  66.91\n",
      "30     life   88   21.43  78.57\n",
      "12      see   85   48.48  51.52\n",
      "\n",
      "##  TOP 20 percentage sensible words\n",
      "        word  all  %_sens   %_ns\n",
      "20      home  142   86.62  13.38\n",
      "26  tomorrow  121   81.82  18.18\n",
      "33   tonight  101   78.22  21.78\n",
      "9       work  187   72.73  27.27\n",
      "10      back  187   70.59  29.41\n",
      "0    propnam  518   62.36  37.64\n",
      "1         go  382   61.26  38.74\n",
      "29     night  115   59.13  40.87\n",
      "13     today  160   57.50  42.50\n",
      "27       got  117   52.99  47.01\n",
      "5         im  206   52.91  47.09\n",
      "3        day  267   51.69  48.31\n",
      "2        get  308   50.00  50.00\n",
      "24      cant  123   49.59  50.41\n",
      "22      come  137   48.91  51.09\n",
      "12       see  165   48.48  51.52\n",
      "8       time  199   44.22  55.78\n",
      "25      feel  122   42.62  57.38\n",
      "15      want  155   40.00  60.00\n",
      "17      good  152   36.84  63.16\n",
      "\n",
      "##  TOP 20 percentage not sensible words\n",
      "     word  all  %_sens   %_ns\n",
      "14  peopl  155   17.42  82.58\n",
      "30   life  112   21.43  78.57\n",
      "6     one  203   23.65  76.35\n",
      "16  think  152   24.34  75.66\n",
      "18   dont  151   25.17  74.83\n",
      "7    love  200   26.00  74.00\n",
      "28   look  115   27.83  72.17\n",
      "19   make  150   28.67  71.33\n",
      "4    like  223   28.70  71.30\n",
      "21   know  140   32.14  67.86\n",
      "11    new  168   32.14  67.86\n",
      "31   need  107   32.71  67.29\n",
      "23  happi  136   33.09  66.91\n",
      "32  first  102   36.27  63.73\n",
      "17   good  152   36.84  63.16\n",
      "15   want  155   40.00  60.00\n",
      "25   feel  122   42.62  57.38\n",
      "8    time  199   44.22  55.78\n",
      "12    see  165   48.48  51.52\n",
      "22   come  137   48.91  51.09\n"
     ]
    }
   ],
   "source": [
    "main_cols = [\"word\", \"all\", \"perc_sens\", \"perc_ns\"]\n",
    "rename_cols = {\"perc_sens\":\"%_sens\", \"perc_ns\":\"%_ns\"}\n",
    "top_values = 20\n",
    "print(\"##  TOP\", top_values, \"all words\")\n",
    "print(data[main_cols][:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"sensible words\")\n",
    "main_cols = [\"word\", \"sens\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data.sort_values(by=\"sens\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"not sensible words\")\n",
    "main_cols = [\"word\", \"ns\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data.sort_values(by=\"ns\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"percentage sensible words\")\n",
    "main_cols = [\"word\", \"all\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data[data[\"all\"]>=100].sort_values(by=\"perc_sens\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"percentage not sensible words\")\n",
    "main_cols = [\"word\", \"all\", \"perc_sens\", \"perc_ns\"]\n",
    "print(data[data[\"all\"]>=100].sort_values(by=\"perc_ns\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [WH+TW] Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words [('im', 1255), ('like', 704), ('want', 647), ('u', 631), ('get', 602), ('go', 566), ('lol', 548), ('love', 497), ('dont', 481), ('girl', 478), ('know', 408), ('good', 381), ('guy', 345), ('day', 339), ('one', 329), ('time', 325), ('feel', 317), ('think', 311), ('need', 307), ('look', 275)] \n",
      "\n",
      "Top words [('im', 1276), ('like', 704), ('u', 663), ('get', 646), ('want', 626), ('love', 536), ('dont', 527), ('girl', 500), ('go', 497), ('lol', 493), ('guy', 380), ('know', 372), ('good', 341), ('one', 341), ('need', 332), ('time', 320), ('feel', 318), ('day', 300), ('look', 292), ('got', 276)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/sample_ann2_01.csv\")\n",
    "data['text_cleaned_stem'] = data['text'].map(lambda x: clean_text(x, remove_sw=True, stem_or_lemma=1))\n",
    "tmp = tmp_func(data, 'text_cleaned_stem')\n",
    "\n",
    "data = pd.read_csv(\"./data/sample_ann2_02.csv\")\n",
    "data['text_cleaned_stem'] = data['text'].map(lambda x: clean_text(x, remove_sw=True, stem_or_lemma=1))\n",
    "tmp = tmp_func(data, 'text_cleaned_stem').append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words [('im', 1255), ('like', 704), ('want', 647), ('u', 631), ('get', 602), ('go', 566), ('lol', 548), ('love', 497), ('dont', 481), ('girl', 478), ('know', 408), ('good', 381), ('guy', 345), ('day', 339), ('one', 329), ('time', 325), ('feel', 317), ('think', 311), ('need', 307), ('look', 275)] \n",
      "\n",
      "Top words [('im', 1276), ('like', 704), ('u', 663), ('get', 646), ('want', 626), ('love', 536), ('dont', 527), ('girl', 500), ('go', 497), ('lol', 493), ('guy', 380), ('know', 372), ('good', 341), ('one', 341), ('need', 332), ('time', 320), ('feel', 318), ('day', 300), ('look', 292), ('got', 276)] \n",
      "\n",
      "Top words [('im', 1321), ('like', 720), ('get', 633), ('u', 593), ('want', 571), ('lol', 556), ('go', 547), ('love', 535), ('girl', 523), ('dont', 475), ('know', 413), ('guy', 406), ('one', 373), ('good', 352), ('day', 347), ('think', 334), ('feel', 328), ('need', 322), ('got', 302), ('time', 293)] \n",
      "\n",
      "Top words [('im', 1321), ('like', 711), ('get', 626), ('u', 598), ('want', 557), ('girl', 524), ('go', 506), ('love', 495), ('lol', 484), ('dont', 438), ('guy', 401), ('good', 391), ('know', 370), ('need', 338), ('feel', 330), ('think', 329), ('one', 325), ('time', 319), ('day', 303), ('make', 301)] \n",
      "\n",
      "Top words [('im', 1279), ('get', 671), ('like', 658), ('u', 648), ('want', 611), ('love', 559), ('girl', 553), ('go', 551), ('dont', 502), ('lol', 498), ('guy', 422), ('know', 395), ('good', 376), ('one', 359), ('time', 337), ('think', 321), ('feel', 318), ('need', 312), ('look', 303), ('day', 295)] \n",
      "\n",
      "Top words [('im', 1283), ('like', 667), ('get', 613), ('want', 607), ('u', 601), ('girl', 538), ('go', 532), ('lol', 517), ('love', 514), ('dont', 487), ('guy', 423), ('know', 406), ('good', 362), ('need', 359), ('feel', 344), ('time', 326), ('think', 318), ('one', 306), ('day', 305), ('cant', 281)] \n",
      "\n",
      "Top words [('im', 1185), ('like', 735), ('want', 649), ('get', 592), ('u', 589), ('lol', 555), ('girl', 502), ('go', 502), ('dont', 490), ('love', 485), ('guy', 432), ('know', 372), ('good', 370), ('need', 345), ('feel', 344), ('think', 328), ('look', 310), ('got', 307), ('one', 307), ('time', 288)] \n",
      "\n",
      "Top words [('im', 1323), ('like', 716), ('get', 622), ('want', 603), ('u', 595), ('lol', 524), ('love', 523), ('go', 517), ('dont', 511), ('girl', 487), ('know', 408), ('guy', 404), ('good', 360), ('one', 353), ('feel', 338), ('got', 313), ('time', 312), ('need', 301), ('think', 300), ('look', 294)] \n",
      "\n",
      "Top words [('im', 1273), ('like', 680), ('get', 651), ('u', 649), ('want', 585), ('girl', 559), ('lol', 547), ('love', 529), ('go', 498), ('dont', 490), ('know', 386), ('guy', 382), ('one', 354), ('think', 342), ('feel', 333), ('day', 321), ('need', 318), ('time', 308), ('got', 303), ('good', 287)] \n",
      "\n",
      "Top words [('im', 1284), ('like', 673), ('get', 636), ('want', 587), ('u', 572), ('go', 532), ('love', 505), ('dont', 496), ('girl', 489), ('lol', 480), ('good', 370), ('guy', 367), ('know', 362), ('need', 354), ('one', 330), ('think', 324), ('time', 319), ('feel', 316), ('got', 302), ('day', 292)] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>all_mean</th>\n",
       "      <th>sens_mean</th>\n",
       "      <th>ns_mean</th>\n",
       "      <th>perc_sens_mean</th>\n",
       "      <th>perc_ns_mean</th>\n",
       "      <th>all_std</th>\n",
       "      <th>sens_std</th>\n",
       "      <th>ns_std</th>\n",
       "      <th>perc_sens_std</th>\n",
       "      <th>perc_ns_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>721.6</td>\n",
       "      <td>558.4</td>\n",
       "      <td>56.378</td>\n",
       "      <td>43.622</td>\n",
       "      <td>40.759457</td>\n",
       "      <td>26.990533</td>\n",
       "      <td>24.949727</td>\n",
       "      <td>1.259151</td>\n",
       "      <td>1.259151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>696.8</td>\n",
       "      <td>369.4</td>\n",
       "      <td>327.4</td>\n",
       "      <td>53.052</td>\n",
       "      <td>46.948</td>\n",
       "      <td>25.633312</td>\n",
       "      <td>14.261058</td>\n",
       "      <td>23.726216</td>\n",
       "      <td>2.226591</td>\n",
       "      <td>2.226591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get</td>\n",
       "      <td>629.2</td>\n",
       "      <td>239.5</td>\n",
       "      <td>389.7</td>\n",
       "      <td>38.060</td>\n",
       "      <td>61.940</td>\n",
       "      <td>23.602260</td>\n",
       "      <td>13.664634</td>\n",
       "      <td>17.269433</td>\n",
       "      <td>1.586485</td>\n",
       "      <td>1.586485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u</td>\n",
       "      <td>613.9</td>\n",
       "      <td>36.9</td>\n",
       "      <td>577.0</td>\n",
       "      <td>6.015</td>\n",
       "      <td>93.985</td>\n",
       "      <td>31.067847</td>\n",
       "      <td>5.724218</td>\n",
       "      <td>30.404678</td>\n",
       "      <td>0.934074</td>\n",
       "      <td>0.934074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want</td>\n",
       "      <td>604.3</td>\n",
       "      <td>473.6</td>\n",
       "      <td>130.7</td>\n",
       "      <td>78.368</td>\n",
       "      <td>21.632</td>\n",
       "      <td>30.485151</td>\n",
       "      <td>26.213652</td>\n",
       "      <td>11.235361</td>\n",
       "      <td>1.597879</td>\n",
       "      <td>1.597879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42286</th>\n",
       "      <td>rescart</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42287</th>\n",
       "      <td>uglyyyyyyi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42288</th>\n",
       "      <td>reschedulei</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42289</th>\n",
       "      <td>uglyperson</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42290</th>\n",
       "      <td>zzzzzzzzzzzzzz</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42291 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  all_mean  sens_mean  ns_mean  perc_sens_mean  \\\n",
       "0                  im    1280.0      721.6    558.4          56.378   \n",
       "1                like     696.8      369.4    327.4          53.052   \n",
       "2                 get     629.2      239.5    389.7          38.060   \n",
       "3                   u     613.9       36.9    577.0           6.015   \n",
       "4                want     604.3      473.6    130.7          78.368   \n",
       "...               ...       ...        ...      ...             ...   \n",
       "42286         rescart       1.0        0.0      1.0           0.000   \n",
       "42287      uglyyyyyyi       1.0        1.0      0.0         100.000   \n",
       "42288     reschedulei       1.0        0.0      1.0           0.000   \n",
       "42289      uglyperson       1.0        0.0      1.0           0.000   \n",
       "42290  zzzzzzzzzzzzzz       1.0        0.0      1.0           0.000   \n",
       "\n",
       "       perc_ns_mean    all_std   sens_std     ns_std  perc_sens_std  \\\n",
       "0            43.622  40.759457  26.990533  24.949727       1.259151   \n",
       "1            46.948  25.633312  14.261058  23.726216       2.226591   \n",
       "2            61.940  23.602260  13.664634  17.269433       1.586485   \n",
       "3            93.985  31.067847   5.724218  30.404678       0.934074   \n",
       "4            21.632  30.485151  26.213652  11.235361       1.597879   \n",
       "...             ...        ...        ...        ...            ...   \n",
       "42286       100.000   0.000000   0.000000   0.000000       0.000000   \n",
       "42287         0.000   0.000000   0.000000   0.000000       0.000000   \n",
       "42288       100.000   0.000000   0.000000   0.000000       0.000000   \n",
       "42289       100.000   0.000000   0.000000   0.000000       0.000000   \n",
       "42290       100.000   0.000000   0.000000   0.000000       0.000000   \n",
       "\n",
       "       perc_ns_std  \n",
       "0         1.259151  \n",
       "1         2.226591  \n",
       "2         1.586485  \n",
       "3         0.934074  \n",
       "4         1.597879  \n",
       "...            ...  \n",
       "42286     0.000000  \n",
       "42287     0.000000  \n",
       "42288     0.000000  \n",
       "42289     0.000000  \n",
       "42290     0.000000  \n",
       "\n",
       "[42291 rows x 11 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "for index in range(10):\n",
    "    data_loc = pd.read_csv(\"./data/sample_ann2_\"+((\"0\"+str(index+1))[-2:])+\".csv\")\n",
    "    data_loc['text_cleaned_stem'] = data_loc['text'].map(lambda x: clean_text(x, remove_sw=True, stem_or_lemma=1))\n",
    "    data = data.append(get_df_stats_2(data_loc, 'text_cleaned_stem'))\n",
    "\n",
    "data = get_df_stats(data).sort_values(by=\"all_mean\", ascending=False).reset_index(drop=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  TOP 20 all words\n",
      "     word  all_mean    all_std  %_sens_mean  %_sens_std  %_ns_mean  %_ns_std\n",
      "0      im    1280.0  40.759457       56.378    1.259151     43.622  1.259151\n",
      "1    like     696.8  25.633312       53.052    2.226591     46.948  2.226591\n",
      "2     get     629.2  23.602260       38.060    1.586485     61.940  1.586485\n",
      "3       u     613.9  31.067847        6.015    0.934074     93.985  0.934074\n",
      "4    want     604.3  30.485151       78.368    1.597879     21.632  1.597879\n",
      "5      go     524.8  24.507595       34.380    1.802745     65.620  1.802745\n",
      "6     lol     520.2  30.124925        7.640    1.083974     92.360  1.083974\n",
      "7    love     517.8  22.812277       58.655    1.820264     41.345  1.820264\n",
      "8    girl     515.3  28.433353       83.171    1.078162     16.829  1.078162\n",
      "9    dont     489.7  23.589310       53.134    2.405462     46.866  2.405462\n",
      "10    guy     396.2  27.454204       83.885    2.203463     16.115  2.203463\n",
      "11   know     389.2  19.147382       49.406    2.587505     50.594  2.587505\n",
      "12   good     359.0  29.070795       27.312    2.929607     72.688  2.929607\n",
      "13    one     337.7  22.246348       51.304    2.981894     48.696  2.981894\n",
      "14   need     328.8  19.993332       47.495    2.567551     52.505  2.567551\n",
      "15   feel     328.6  11.067470       66.907    2.205992     33.093  2.205992\n",
      "16  think     318.3  18.956383       53.259    2.514073     46.741  2.514073\n",
      "17   time     314.7  15.026273       43.844    2.954857     56.156  2.954857\n",
      "18    day     305.8  24.307292       36.064    3.329735     63.936  3.329735\n",
      "19    got     293.2  14.845875       26.284    3.296669     73.716  3.296669\n",
      "\n",
      "##  TOP 20 sensible words\n",
      "      word  sens_mean   sens_std  %_sens_mean  %_sens_std  %_ns_mean  %_ns_std\n",
      "0       im      721.6  26.990533       56.378    1.259151     43.622  1.259151\n",
      "4     want      473.6  26.213652       78.368    1.597879     21.632  1.597879\n",
      "8     girl      428.5  22.790105       83.171    1.078162     16.829  1.078162\n",
      "1     like      369.4  14.261058       53.052    2.226591     46.948  2.226591\n",
      "10     guy      332.4  25.530809       83.885    2.203463     16.115  2.203463\n",
      "7     love      303.9  20.085650       58.655    1.820264     41.345  1.820264\n",
      "9     dont      260.1  15.552420       53.134    2.405462     46.866  2.405462\n",
      "2      get      239.5  13.664634       38.060    1.586485     61.940  1.586485\n",
      "15    feel      219.9  11.415876       66.907    2.205992     33.093  2.205992\n",
      "22  friend      202.4   5.146736       76.349    1.860994     23.651  1.860994\n",
      "11    know      192.6  18.081298       49.406    2.587505     50.594  2.587505\n",
      "5       go      180.3  10.456789       34.380    1.802745     65.620  1.802745\n",
      "13     one      173.2  14.420664       51.304    2.981894     48.696  2.981894\n",
      "27    talk      169.9  10.887812       72.566    2.871725     27.434  2.871725\n",
      "16   think      169.4  11.305849       53.259    2.514073     46.741  2.514073\n",
      "25   peopl      157.9  13.947919       64.068    1.781365     35.932  1.781365\n",
      "14    need      156.3  14.345344       47.495    2.567551     52.505  2.567551\n",
      "20    look      153.9   8.569325       54.181    2.652866     45.819  2.652866\n",
      "30   wanna      149.8  11.321563       70.938    3.012883     29.062  3.012883\n",
      "40  someon      139.8  16.088643       79.843    1.895515     20.157  1.895515\n",
      "\n",
      "##  TOP 20 not sensible words\n",
      "     word  ns_mean     ns_std  %_sens_mean  %_sens_std  %_ns_mean  %_ns_std\n",
      "3       u    577.0  30.404678        6.015    0.934074     93.985  0.934074\n",
      "0      im    558.4  24.949727       56.378    1.259151     43.622  1.259151\n",
      "6     lol    480.4  27.043997        7.640    1.083974     92.360  1.083974\n",
      "2     get    389.7  17.269433       38.060    1.586485     61.940  1.586485\n",
      "5      go    344.5  21.114766       34.380    1.802745     65.620  1.802745\n",
      "1    like    327.4  23.726216       53.052    2.226591     46.948  2.226591\n",
      "12   good    260.7  20.591530       27.312    2.929607     72.688  2.929607\n",
      "9    dont    229.6  17.821336       53.134    2.405462     46.866  2.405462\n",
      "19    got    216.2  15.718354       26.284    3.296669     73.716  3.296669\n",
      "7    love    213.9   9.677580       58.655    1.820264     41.345  1.820264\n",
      "11   know    196.6   7.748835       49.406    2.587505     50.594  2.587505\n",
      "18    day    195.4  17.417743       36.064    3.329735     63.936  3.329735\n",
      "29      2    185.4  12.375603       14.024    2.497364     85.976  2.497364\n",
      "17   time    176.7  12.302213       43.844    2.954857     56.156  2.954857\n",
      "14   need    172.5  11.559027       47.495    2.567551     52.505  2.567551\n",
      "23   work    170.8  10.538817       35.157    2.205720     64.843  2.205720\n",
      "13    one    164.5  15.443445       51.304    2.981894     48.696  2.981894\n",
      "16  think    148.9  13.127156       53.259    2.514073     46.741  2.514073\n",
      "21   make    141.9  10.928556       48.224    2.967341     51.776  2.967341\n",
      "28    see    139.1   6.261878       40.162    3.720913     59.838  3.720913\n",
      "\n",
      "##  TOP 20 percentage sensible words\n",
      "         word  all_mean    all_std  %_sens_mean  %_sens_std  %_ns_mean  \\\n",
      "91       chat     101.1   7.823753       96.591    1.488306      3.409   \n",
      "73  boyfriend     117.3  15.944696       95.194    1.963207      4.806   \n",
      "62        sex     136.8  13.172362       91.404    2.779713      8.596   \n",
      "10        guy     396.2  27.454204       83.885    2.203463     16.115   \n",
      "46      anyon     162.8   8.390471       83.323    2.128834     16.677   \n",
      "8        girl     515.3  28.433353       83.171    1.078162     16.829   \n",
      "82       bore     110.3  11.614646       80.319    4.183880     19.681   \n",
      "40     someon     175.0  18.879736       79.843    1.895515     20.157   \n",
      "4        want     604.3  30.485151       78.368    1.597879     21.632   \n",
      "22     friend     265.2   8.011103       76.349    1.860994     23.651   \n",
      "27       talk     234.2  13.314987       72.566    2.871725     27.434   \n",
      "30      wanna     211.1  12.077987       70.938    3.012883     29.062   \n",
      "80       ladi     110.9  13.253511       69.505    4.162636     30.495   \n",
      "61       wish     139.9  12.573782       69.258    3.703310     30.742   \n",
      "87       find     104.8  11.773794       67.731    3.183452     32.269   \n",
      "51        ive     151.5   9.021579       67.298    3.265179     32.702   \n",
      "15       feel     328.6  11.067470       66.907    2.205992     33.093   \n",
      "41       hate     174.9  11.019679       65.669    4.024032     34.331   \n",
      "25      peopl     246.8  24.760183       64.068    1.781365     35.932   \n",
      "49       life     156.3  15.727188       63.266    3.391398     36.734   \n",
      "\n",
      "    %_ns_std  \n",
      "91  1.488306  \n",
      "73  1.963207  \n",
      "62  2.779713  \n",
      "10  2.203463  \n",
      "46  2.128834  \n",
      "8   1.078162  \n",
      "82  4.183880  \n",
      "40  1.895515  \n",
      "4   1.597879  \n",
      "22  1.860994  \n",
      "27  2.871725  \n",
      "30  3.012883  \n",
      "80  4.162636  \n",
      "61  3.703310  \n",
      "87  3.183452  \n",
      "51  3.265179  \n",
      "15  2.205992  \n",
      "41  4.024032  \n",
      "25  1.781365  \n",
      "49  3.391398  \n",
      "\n",
      "##  TOP 20 percentage not sensible words\n",
      "     word  all_mean    all_std  %_sens_mean  %_sens_std  %_ns_mean  %_ns_std\n",
      "3       u     613.9  31.067847        6.015    0.934074     93.985  0.934074\n",
      "66     ur     125.1  17.704049        6.626    2.984606     93.374  2.984606\n",
      "6     lol     520.2  30.124925        7.640    1.083974     92.360  1.083974\n",
      "67      n     124.7  13.081369       10.283    3.933139     89.717  3.933139\n",
      "71  thank     117.7  12.037442       13.047    3.048931     86.953  3.048931\n",
      "29      2     215.6  12.011106       14.024    2.497364     85.976  2.497364\n",
      "84  gonna     107.1  15.117686       18.430    2.609291     81.570  2.609291\n",
      "83   well     108.9  12.853447       20.733    4.126952     79.267  4.126952\n",
      "53   that     149.9  10.774971       22.766    3.334014     77.234  3.334014\n",
      "52   shit     151.3  12.383232       25.573    3.926313     74.427  3.926313\n",
      "19    got     293.2  14.845875       26.284    3.296669     73.716  3.296669\n",
      "35    new     187.4  10.221981       26.467    3.875828     73.533  3.875828\n",
      "12   good     359.0  29.070795       27.312    2.929607     72.688  2.929607\n",
      "90   wait     102.4  12.954622       28.956    4.589873     71.044  4.589873\n",
      "32   come     192.4  16.668000       30.192    3.153925     69.808  3.153925\n",
      "69   home     120.3  11.823235       31.426    3.120617     68.574  3.120617\n",
      "42  right     172.7  11.888837       31.848    3.715466     68.152  3.715466\n",
      "45  watch     165.4  16.419501       32.301    2.618670     67.699  2.618670\n",
      "34   back     191.5  14.001984       32.994    3.001819     67.006  3.001819\n",
      "75   call     116.0  10.934146       33.301    4.306603     66.699  4.306603\n"
     ]
    }
   ],
   "source": [
    "main_cols = [\"word\", \"all_mean\", \"all_std\", \"perc_sens_mean\", \"perc_sens_std\", \"perc_ns_mean\", \"perc_ns_std\"]\n",
    "rename_cols = {\"perc_sens_mean\":\"%_sens_mean\", \"perc_sens_std\":\"%_sens_std\", \"perc_ns_mean\":\"%_ns_mean\", \"perc_ns_std\":\"%_ns_std\"}\n",
    "top_values = 20\n",
    "print(\"##  TOP\", top_values, \"all words\")\n",
    "print(data[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"sensible words\")\n",
    "main_cols = [\"word\", \"sens_mean\", \"sens_std\", \"perc_sens_mean\", \"perc_sens_std\", \"perc_ns_mean\", \"perc_ns_std\"]\n",
    "print(data.sort_values(by=\"sens_mean\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"not sensible words\")\n",
    "main_cols = [\"word\", \"ns_mean\", \"ns_std\", \"perc_sens_mean\", \"perc_sens_std\", \"perc_ns_mean\", \"perc_ns_std\"]\n",
    "print(data.sort_values(by=\"ns_mean\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"percentage sensible words\")\n",
    "main_cols = [\"word\", \"all_mean\", \"all_std\", \"perc_sens_mean\", \"perc_sens_std\", \"perc_ns_mean\", \"perc_ns_std\"]\n",
    "print(data[data[\"all_mean\"]>=100].sort_values(by=\"perc_sens_mean\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "print(\"\\n##  TOP\", top_values, \"percentage not sensible words\")\n",
    "main_cols = [\"word\", \"all_mean\", \"all_std\", \"perc_sens_mean\", \"perc_sens_std\", \"perc_ns_mean\", \"perc_ns_std\"]\n",
    "print(data[data[\"all_mean\"]>=100].sort_values(by=\"perc_ns_mean\", ascending=False)[main_cols].rename(columns=rename_cols)[:top_values])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sets: training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils import get_data, get_two_classes, my_train_val_test_split\n",
    "\n",
    "# training\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## agreement on 2 annotators\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## agreement on 3 annotators\n",
      "Size of: training -> 3036 (75%), val -> 606 (14%), test -> 404 (9%)\n"
     ]
    }
   ],
   "source": [
    "for ann in [2,3]:\n",
    "    print(\"## agreement on\", ann, \"annotators\")\n",
    "    # read data\n",
    "    data = get_data(\"./data/annotation_results.csv\", lim=ann)\n",
    "    data = get_two_classes(data)\n",
    "    # create train, validation and test sets\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = my_train_val_test_split(data['text'].values, data['class'].values, validation_ratio, test_ratio, 512)\n",
    "    print(\"Size of: training -> \"+str(len(x_train))+\" (\"+str(int(100*len(x_train)/(len(data))))+\"%), val -> \"+str(len(x_val))+\" (\"+str(int(100*len(x_val)/(len(data))))+\"%), test -> \"+str(len(x_test))+\" (\"+str(int(100*len(x_test)/(len(data))))+\"%)\")\n",
    "    # save sets\n",
    "    pd.DataFrame({\"text\": x_train, \"class\": y_train}).to_csv(\"./data/annotation_results__ann\"+str(ann)+\"_training.csv\", index=False)\n",
    "    pd.DataFrame({\"text\": x_val, \"class\": y_val}).to_csv(\"./data/annotation_results__ann\"+str(ann)+\"_validation.csv\", index=False)\n",
    "    pd.DataFrame({\"text\": x_test, \"class\": y_test}).to_csv(\"./data/annotation_results__ann\"+str(ann)+\"_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## sample 1\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 2\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 3\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 4\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 5\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 6\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 7\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 8\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 9\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n",
      "## sample 10\n",
      "Size of: training -> 6575 (75%), val -> 1314 (14%), test -> 876 (9%)\n"
     ]
    }
   ],
   "source": [
    "for index in range(10):\n",
    "    print(\"## sample\", (index+1))\n",
    "    # read data\n",
    "    pathname = \"./data/sample_ann2_\"+((\"0\"+str(index+1))[-2:])\n",
    "    data = pd.read_csv(pathname+\".csv\")\n",
    "    # create train, validation and test sets\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = my_train_val_test_split(data['text'].values, data['class'].values, validation_ratio, test_ratio, 512)\n",
    "    print(\"Size of: training -> \"+str(len(x_train))+\" (\"+str(int(100*len(x_train)/(len(data))))+\"%), val -> \"+str(len(x_val))+\" (\"+str(int(100*len(x_val)/(len(data))))+\"%), test -> \"+str(len(x_test))+\" (\"+str(int(100*len(x_test)/(len(data))))+\"%)\")\n",
    "    # save sets\n",
    "    pd.DataFrame({\"text\": x_train, \"class\": y_train}).to_csv(pathname+\"_training.csv\", index=False)\n",
    "    pd.DataFrame({\"text\": x_val, \"class\": y_val}).to_csv(pathname+\"_validation.csv\", index=False)\n",
    "    pd.DataFrame({\"text\": x_test, \"class\": y_test}).to_csv(pathname+\"_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
